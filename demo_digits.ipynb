{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MicroGrad demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import Neuron, Layer, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# make up a dataset\n",
    "from sklearn.datasets import make_moons, make_blobs, load_digits, fetch_openml\n",
    "#X, y = fetch_openml('mnist_784',return_X_y=True)\n",
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = y.astype(np.int)\n",
    "y_oh = np.zeros((y.shape[0], y.max()+1))\n",
    "y_oh[np.arange(y.shape[0]),y] = 1\n",
    "y_oh = y_oh*1.1 - 0.1 # make y_oh be -0.1 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12ecbb110>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X.shape, y_oh.shape)\n",
    "plt.imshow(X[0].reshape(8,8), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP of [Layer of [ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64), ReLUNeuron(64)], Layer of [LinearNeuron(16), LinearNeuron(16), LinearNeuron(16), LinearNeuron(16), LinearNeuron(16), LinearNeuron(16), LinearNeuron(16), LinearNeuron(16), LinearNeuron(16), LinearNeuron(16)]]\n",
      "number of parameters 1210\n"
     ]
    }
   ],
   "source": [
    "# initialize a model \n",
    "model = MLP(64, [16, 10]) # 2-layer neural network\n",
    "print(model)\n",
    "print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=64.00688247438974, grad=0) 0.1296605453533667\n"
     ]
    }
   ],
   "source": [
    "# argmax\n",
    "def argmax(values):\n",
    "    return max(enumerate(values), key=lambda x: x[1])[0]\n",
    "\n",
    "# loss function\n",
    "def loss(batch_size=None):\n",
    "    # inline DataLoader :)\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y_oh\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y_oh[ri]\n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # svm \"max-margin\" loss\n",
    "    losses = [sum(((1 + -yic*scoreic).relu() for yic,scoreic in zip(yi, scorei))) for yi, scorei in zip(yb, scores)]\n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "    # L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    total_loss = data_loss + reg_loss\n",
    "    \n",
    "    # also get accuracy\n",
    "    accuracy = [yi.argmax() == argmax([scoreic.data for scoreic in scorei]) for yi, scorei in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 56.69842273085016, accuracy 14.0625%\n",
      "step 1 loss 37.90824259965874, accuracy 19.53125%\n",
      "step 2 loss 25.249149698360693, accuracy 25.78125%\n",
      "step 3 loss 11.351464159095478, accuracy 36.71875%\n",
      "step 4 loss 11.406599920156726, accuracy 21.09375%\n",
      "step 5 loss 10.274761907965976, accuracy 21.09375%\n",
      "step 6 loss 9.264822422031047, accuracy 25.0%\n",
      "step 7 loss 9.345076161491319, accuracy 29.6875%\n",
      "step 8 loss 8.668700463731506, accuracy 27.34375%\n",
      "step 9 loss 8.403121814324617, accuracy 31.25%\n",
      "step 10 loss 8.396698927574015, accuracy 28.90625%\n",
      "step 11 loss 7.8010737812719695, accuracy 25.0%\n",
      "step 12 loss 6.90986105248893, accuracy 24.21875%\n",
      "step 13 loss 7.975404871945863, accuracy 28.125%\n",
      "step 14 loss 8.909172016504721, accuracy 34.375%\n",
      "step 15 loss 8.798911775964157, accuracy 29.6875%\n",
      "step 16 loss 7.718016852618044, accuracy 30.46875%\n",
      "step 17 loss 6.527810471850269, accuracy 45.3125%\n",
      "step 18 loss 6.3277266249978945, accuracy 28.90625%\n",
      "step 19 loss 6.063916892197813, accuracy 32.03125%\n",
      "step 20 loss 6.2120387940089214, accuracy 32.03125%\n",
      "step 21 loss 5.34943197013611, accuracy 46.09375%\n",
      "step 22 loss 5.360902836299114, accuracy 36.71875%\n",
      "step 23 loss 6.233782673247174, accuracy 31.25%\n",
      "step 24 loss 6.629480366485576, accuracy 27.34375%\n",
      "step 25 loss 6.980001155165668, accuracy 41.40625%\n",
      "step 26 loss 5.530976014004772, accuracy 42.96875%\n",
      "step 27 loss 5.50178511848857, accuracy 35.15625%\n",
      "step 28 loss 4.99270716293917, accuracy 47.65625%\n",
      "step 29 loss 4.93331401162488, accuracy 45.3125%\n",
      "step 30 loss 5.172222280036566, accuracy 47.65625%\n",
      "step 31 loss 5.653752724494715, accuracy 26.5625%\n",
      "step 32 loss 4.764737419919635, accuracy 40.625%\n",
      "step 33 loss 4.572397322886338, accuracy 53.125%\n",
      "step 34 loss 4.504308709938468, accuracy 52.34375%\n",
      "step 35 loss 4.931378668898897, accuracy 51.5625%\n",
      "step 36 loss 4.570928733711075, accuracy 53.125%\n",
      "step 37 loss 4.609527465114594, accuracy 46.875%\n",
      "step 38 loss 4.017902461101131, accuracy 50.0%\n",
      "step 39 loss 4.831075124589859, accuracy 46.09375%\n",
      "step 40 loss 4.217423138524784, accuracy 49.21875%\n",
      "step 41 loss 5.213512460599012, accuracy 45.3125%\n",
      "step 42 loss 4.736775337448376, accuracy 48.4375%\n",
      "step 43 loss 4.58385754181896, accuracy 39.84375%\n",
      "step 44 loss 5.033388572059352, accuracy 39.0625%\n",
      "step 45 loss 4.150336319167732, accuracy 55.46875%\n",
      "step 46 loss 4.1532283369597005, accuracy 53.125%\n",
      "step 47 loss 4.301567437952713, accuracy 52.34375%\n",
      "step 48 loss 4.027147937008369, accuracy 56.25%\n",
      "step 49 loss 3.8792752872671294, accuracy 54.6875%\n",
      "step 50 loss 4.5337940943880435, accuracy 32.03125%\n",
      "step 51 loss 4.213427981545586, accuracy 42.96875%\n",
      "step 52 loss 4.142525775866434, accuracy 48.4375%\n",
      "step 53 loss 4.232098121280005, accuracy 50.78125%\n",
      "step 54 loss 4.282542100654668, accuracy 54.6875%\n",
      "step 55 loss 4.66014025641862, accuracy 46.09375%\n",
      "step 56 loss 4.422450511654553, accuracy 51.5625%\n",
      "step 57 loss 4.224762482535683, accuracy 49.21875%\n",
      "step 58 loss 4.110730095334767, accuracy 50.78125%\n",
      "step 59 loss 4.096466757117663, accuracy 57.8125%\n",
      "step 60 loss 4.461639864121954, accuracy 46.875%\n",
      "step 61 loss 5.149666699299625, accuracy 50.78125%\n",
      "step 62 loss 4.312262452281789, accuracy 48.4375%\n",
      "step 63 loss 3.9430169227222556, accuracy 54.6875%\n",
      "step 64 loss 4.158971998932615, accuracy 46.09375%\n",
      "step 65 loss 3.545932688589077, accuracy 71.09375%\n",
      "step 66 loss 4.347547872686615, accuracy 54.6875%\n",
      "step 67 loss 3.959375809593008, accuracy 52.34375%\n",
      "step 68 loss 3.65254923693825, accuracy 47.65625%\n",
      "step 69 loss 3.5234779585828457, accuracy 55.46875%\n",
      "step 70 loss 3.607257385358365, accuracy 60.9375%\n",
      "step 71 loss 3.7287270491777385, accuracy 61.71875%\n",
      "step 72 loss 3.920638861667718, accuracy 55.46875%\n",
      "step 73 loss 5.095670391297613, accuracy 32.8125%\n",
      "step 74 loss 4.487245603072926, accuracy 41.40625%\n",
      "step 75 loss 3.817828289900549, accuracy 50.78125%\n",
      "step 76 loss 4.279728822289205, accuracy 53.125%\n",
      "step 77 loss 4.503760247906682, accuracy 53.125%\n",
      "step 78 loss 4.468047758452562, accuracy 40.625%\n",
      "step 79 loss 3.9015120053326457, accuracy 54.6875%\n",
      "step 80 loss 3.8655701208549185, accuracy 53.125%\n",
      "step 81 loss 3.528096727753189, accuracy 76.5625%\n",
      "step 82 loss 3.9795880256087943, accuracy 46.875%\n",
      "step 83 loss 4.102306378750337, accuracy 39.84375%\n",
      "step 84 loss 4.051916059439471, accuracy 60.15625%\n",
      "step 85 loss 4.095443475191012, accuracy 57.8125%\n",
      "step 86 loss 3.4888488645351137, accuracy 61.71875%\n",
      "step 87 loss 3.6209792494758375, accuracy 59.375%\n",
      "step 88 loss 3.3408970231287998, accuracy 63.28125%\n",
      "step 89 loss 3.6835367010149356, accuracy 59.375%\n",
      "step 90 loss 4.482535866380942, accuracy 53.90625%\n",
      "step 91 loss 3.7941511061952236, accuracy 46.09375%\n",
      "step 92 loss 3.410629087530815, accuracy 71.09375%\n",
      "step 93 loss 3.260925000732064, accuracy 54.6875%\n",
      "step 94 loss 4.393784989493278, accuracy 50.0%\n",
      "step 95 loss 4.328146858032064, accuracy 53.90625%\n",
      "step 96 loss 3.5396980787457886, accuracy 56.25%\n",
      "step 97 loss 3.2640586217207237, accuracy 72.65625%\n",
      "step 98 loss 5.152185290395639, accuracy 46.09375%\n",
      "step 99 loss 4.711012887645665, accuracy 49.21875%\n",
      "step 100 loss 3.9280516584709138, accuracy 56.25%\n",
      "step 101 loss 3.4680536337618744, accuracy 79.6875%\n",
      "step 102 loss 3.3421956202950036, accuracy 70.3125%\n",
      "step 103 loss 3.145117811640202, accuracy 74.21875%\n",
      "step 104 loss 2.855732464213635, accuracy 67.96875%\n",
      "step 105 loss 3.34020835217141, accuracy 64.84375%\n",
      "step 106 loss 3.8008869323339316, accuracy 60.9375%\n",
      "step 107 loss 3.333465992239474, accuracy 70.3125%\n",
      "step 108 loss 3.5699133151451843, accuracy 58.59375%\n",
      "step 109 loss 3.8035294900594896, accuracy 57.03125%\n",
      "step 110 loss 3.00804283057578, accuracy 65.625%\n",
      "step 111 loss 3.1014584788730324, accuracy 71.875%\n",
      "step 112 loss 3.1246153611436323, accuracy 62.5%\n",
      "step 113 loss 2.704176563448086, accuracy 84.375%\n",
      "step 114 loss 3.4195067294412027, accuracy 54.6875%\n",
      "step 115 loss 2.971075196651092, accuracy 72.65625%\n",
      "step 116 loss 3.2901082652765057, accuracy 66.40625%\n",
      "step 117 loss 3.35598019862997, accuracy 60.9375%\n",
      "step 118 loss 3.5418802244677012, accuracy 71.875%\n",
      "step 119 loss 3.3699672898181885, accuracy 65.625%\n",
      "step 120 loss 2.788123913544322, accuracy 73.4375%\n",
      "step 121 loss 3.416521617193995, accuracy 64.0625%\n",
      "step 122 loss 3.2525095235361916, accuracy 56.25%\n",
      "step 123 loss 3.4113306128566485, accuracy 76.5625%\n",
      "step 124 loss 3.779038647114922, accuracy 61.71875%\n",
      "step 125 loss 3.693393429006132, accuracy 76.5625%\n",
      "step 126 loss 3.15522580849853, accuracy 62.5%\n",
      "step 127 loss 3.1429763814302816, accuracy 57.03125%\n",
      "step 128 loss 3.727187651536215, accuracy 45.3125%\n",
      "step 129 loss 2.8327210484425382, accuracy 65.625%\n",
      "step 130 loss 2.4291779859285905, accuracy 72.65625%\n",
      "step 131 loss 3.01492317995252, accuracy 71.875%\n",
      "step 132 loss 3.740253847069622, accuracy 59.375%\n",
      "step 133 loss 3.1135497172061606, accuracy 75.0%\n",
      "step 134 loss 3.0025016423578093, accuracy 79.6875%\n",
      "step 135 loss 3.0324621351436414, accuracy 63.28125%\n",
      "step 136 loss 3.2999003221079932, accuracy 70.3125%\n",
      "step 137 loss 3.03259954666518, accuracy 56.25%\n",
      "step 138 loss 3.737741002351099, accuracy 64.84375%\n",
      "step 139 loss 2.956359296397493, accuracy 71.09375%\n",
      "step 140 loss 2.584212049041718, accuracy 76.5625%\n",
      "step 141 loss 3.0497993586010366, accuracy 71.09375%\n",
      "step 142 loss 3.0689192892216797, accuracy 66.40625%\n",
      "step 143 loss 2.985067165196076, accuracy 64.0625%\n",
      "step 144 loss 2.5615793655810735, accuracy 73.4375%\n",
      "step 145 loss 3.131579967258773, accuracy 65.625%\n",
      "step 146 loss 2.777399663586651, accuracy 64.84375%\n",
      "step 147 loss 2.7498902900398594, accuracy 64.84375%\n",
      "step 148 loss 2.621297569868759, accuracy 70.3125%\n",
      "step 149 loss 2.7584853512457492, accuracy 75.0%\n",
      "step 150 loss 2.8201550340391996, accuracy 76.5625%\n",
      "step 151 loss 2.5379713796323515, accuracy 73.4375%\n",
      "step 152 loss 2.6153849570742898, accuracy 71.875%\n",
      "step 153 loss 2.9152231013838623, accuracy 73.4375%\n",
      "step 154 loss 1.9241081755589238, accuracy 82.8125%\n",
      "step 155 loss 2.659482965621474, accuracy 66.40625%\n",
      "step 156 loss 3.05479520947245, accuracy 57.8125%\n",
      "step 157 loss 2.611065613682534, accuracy 81.25%\n",
      "step 158 loss 2.6783683722600093, accuracy 85.9375%\n",
      "step 159 loss 3.118823390580526, accuracy 66.40625%\n",
      "step 160 loss 2.7508031623865272, accuracy 72.65625%\n",
      "step 161 loss 2.2943030272827523, accuracy 82.03125%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 162 loss 2.3645174100417186, accuracy 85.15625%\n",
      "step 163 loss 1.9762744812192548, accuracy 83.59375%\n",
      "step 164 loss 1.9460810217795719, accuracy 82.03125%\n",
      "step 165 loss 3.1758259170424017, accuracy 71.875%\n",
      "step 166 loss 2.6205436224134058, accuracy 64.84375%\n",
      "step 167 loss 2.404855911900625, accuracy 72.65625%\n",
      "step 168 loss 2.3158536385324333, accuracy 81.25%\n",
      "step 169 loss 2.0769298006406776, accuracy 81.25%\n",
      "step 170 loss 2.4662849915329192, accuracy 78.125%\n",
      "step 171 loss 2.6259693365773735, accuracy 75.78125%\n",
      "step 172 loss 2.990187636921771, accuracy 76.5625%\n",
      "step 173 loss 2.6652561231140677, accuracy 72.65625%\n",
      "step 174 loss 2.664970413331088, accuracy 71.09375%\n",
      "step 175 loss 2.309906480645282, accuracy 79.6875%\n",
      "step 176 loss 2.6361216911010894, accuracy 64.84375%\n",
      "step 177 loss 2.6097170243957106, accuracy 70.3125%\n",
      "step 178 loss 2.2776435193249975, accuracy 75.0%\n",
      "step 179 loss 2.412295125382963, accuracy 83.59375%\n",
      "step 180 loss 2.5111388583928953, accuracy 74.21875%\n",
      "step 181 loss 1.6716080977756562, accuracy 85.9375%\n",
      "step 182 loss 2.2817645945133433, accuracy 85.15625%\n",
      "step 183 loss 2.536725784777612, accuracy 77.34375%\n",
      "step 184 loss 1.8827107740555142, accuracy 85.15625%\n",
      "step 185 loss 2.500095344366338, accuracy 64.0625%\n",
      "step 186 loss 1.9927961037642443, accuracy 71.875%\n",
      "step 187 loss 1.9928363219692922, accuracy 87.5%\n",
      "step 188 loss 2.7608492652361196, accuracy 61.71875%\n",
      "step 189 loss 2.291411949517673, accuracy 74.21875%\n",
      "step 190 loss 2.420261223094952, accuracy 77.34375%\n",
      "step 191 loss 1.9179034310542438, accuracy 81.25%\n",
      "step 192 loss 1.546829056788841, accuracy 83.59375%\n",
      "step 193 loss 1.9894311717925544, accuracy 81.25%\n",
      "step 194 loss 1.9195609939977814, accuracy 83.59375%\n",
      "step 195 loss 2.0301882539595892, accuracy 85.15625%\n",
      "step 196 loss 1.9598712338943516, accuracy 78.90625%\n",
      "step 197 loss 1.9211727587658494, accuracy 84.375%\n",
      "step 198 loss 2.674836511453768, accuracy 79.6875%\n",
      "step 199 loss 2.5361966573824604, accuracy 67.1875%\n",
      "step 200 loss 1.8402410745510849, accuracy 82.8125%\n",
      "step 201 loss 2.312078740331886, accuracy 79.6875%\n",
      "step 202 loss 2.3242713473335472, accuracy 78.125%\n",
      "step 203 loss 2.633118005517577, accuracy 77.34375%\n",
      "step 204 loss 1.6550006372360306, accuracy 85.15625%\n",
      "step 205 loss 1.870479451436964, accuracy 82.8125%\n",
      "step 206 loss 2.7377296218077416, accuracy 81.25%\n",
      "step 207 loss 2.5449181313881963, accuracy 67.96875%\n",
      "step 208 loss 2.275267349697613, accuracy 72.65625%\n",
      "step 209 loss 2.2019521252243153, accuracy 85.15625%\n",
      "step 210 loss 2.193818737753166, accuracy 82.8125%\n",
      "step 211 loss 2.1217864341075283, accuracy 76.5625%\n",
      "step 212 loss 1.6218730568473387, accuracy 86.71875%\n",
      "step 213 loss 1.8730885097279648, accuracy 84.375%\n",
      "step 214 loss 2.120146514635232, accuracy 80.46875%\n",
      "step 215 loss 1.448756115647689, accuracy 89.0625%\n",
      "step 216 loss 1.98559243172172, accuracy 86.71875%\n",
      "step 217 loss 1.988255522984398, accuracy 76.5625%\n",
      "step 218 loss 1.8425811631849631, accuracy 78.125%\n",
      "step 219 loss 1.6875020713550155, accuracy 82.03125%\n",
      "step 220 loss 2.0162901229091674, accuracy 74.21875%\n",
      "step 221 loss 2.0939563221248467, accuracy 78.125%\n",
      "step 222 loss 3.9462484456638625, accuracy 55.46875%\n",
      "step 223 loss 2.434361076500243, accuracy 71.09375%\n",
      "step 224 loss 2.4032536208101956, accuracy 72.65625%\n",
      "step 225 loss 2.481204774672107, accuracy 85.15625%\n",
      "step 226 loss 1.9417186754902318, accuracy 78.125%\n",
      "step 227 loss 1.6321671098389994, accuracy 90.625%\n",
      "step 228 loss 2.0454886738049116, accuracy 78.125%\n",
      "step 229 loss 2.2033727310146918, accuracy 80.46875%\n",
      "step 230 loss 2.1158831481221627, accuracy 82.03125%\n",
      "step 231 loss 2.1689620204597584, accuracy 79.6875%\n",
      "step 232 loss 1.9475034846837076, accuracy 82.8125%\n",
      "step 233 loss 1.699567527635032, accuracy 82.8125%\n",
      "step 234 loss 1.8791247151629507, accuracy 82.8125%\n",
      "step 235 loss 1.5801021162347018, accuracy 85.9375%\n",
      "step 236 loss 1.5612361105410042, accuracy 88.28125%\n",
      "step 237 loss 2.140263632297454, accuracy 69.53125%\n",
      "step 238 loss 2.231148002724973, accuracy 81.25%\n",
      "step 239 loss 1.4169403227844448, accuracy 88.28125%\n",
      "step 240 loss 1.9554185558715795, accuracy 84.375%\n",
      "step 241 loss 2.2696536571121975, accuracy 71.875%\n",
      "step 242 loss 1.838387238936448, accuracy 79.6875%\n",
      "step 243 loss 1.6952298957643672, accuracy 75.78125%\n",
      "step 244 loss 2.160854775853984, accuracy 76.5625%\n",
      "step 245 loss 1.6088248437994264, accuracy 85.9375%\n",
      "step 246 loss 1.8633424570070662, accuracy 78.125%\n",
      "step 247 loss 1.783093334881784, accuracy 86.71875%\n",
      "step 248 loss 1.6228618103095178, accuracy 88.28125%\n",
      "step 249 loss 2.124471078218969, accuracy 83.59375%\n",
      "step 250 loss 2.2358643880899876, accuracy 82.8125%\n",
      "step 251 loss 2.0476087032264827, accuracy 78.125%\n",
      "step 252 loss 2.0862977713199657, accuracy 85.9375%\n",
      "step 253 loss 1.7960021116007334, accuracy 75.78125%\n",
      "step 254 loss 2.0346826128696343, accuracy 85.15625%\n",
      "step 255 loss 1.9314995536626698, accuracy 82.03125%\n",
      "step 256 loss 1.4950651711424165, accuracy 81.25%\n",
      "step 257 loss 1.7543509648809048, accuracy 80.46875%\n",
      "step 258 loss 2.119672321824759, accuracy 72.65625%\n",
      "step 259 loss 1.8897180691104267, accuracy 76.5625%\n",
      "step 260 loss 1.827407924276057, accuracy 78.125%\n",
      "step 261 loss 2.0477028501496513, accuracy 78.125%\n",
      "step 262 loss 2.0166293301118627, accuracy 76.5625%\n",
      "step 263 loss 1.87193001800051, accuracy 78.90625%\n",
      "step 264 loss 1.3007445895127976, accuracy 85.15625%\n",
      "step 265 loss 1.758797706914264, accuracy 78.90625%\n",
      "step 266 loss 1.5034862975981953, accuracy 86.71875%\n",
      "step 267 loss 1.2216761864239696, accuracy 88.28125%\n",
      "step 268 loss 2.0293314621329053, accuracy 82.03125%\n",
      "step 269 loss 1.2862636583400697, accuracy 85.15625%\n",
      "step 270 loss 1.9148094468395112, accuracy 85.15625%\n",
      "step 271 loss 1.5751934766925626, accuracy 79.6875%\n",
      "step 272 loss 1.5773006730039763, accuracy 88.28125%\n",
      "step 273 loss 1.8514146163452372, accuracy 80.46875%\n",
      "step 274 loss 1.8485989420425457, accuracy 85.9375%\n",
      "step 275 loss 2.0566051369468785, accuracy 77.34375%\n",
      "step 276 loss 2.0887131923098194, accuracy 81.25%\n",
      "step 277 loss 1.306611598995024, accuracy 89.0625%\n",
      "step 278 loss 2.248502314058043, accuracy 87.5%\n",
      "step 279 loss 1.6757380821070695, accuracy 75.0%\n",
      "step 280 loss 1.4101037160270475, accuracy 87.5%\n",
      "step 281 loss 1.779990332637773, accuracy 81.25%\n",
      "step 282 loss 1.7981022496961256, accuracy 75.78125%\n",
      "step 283 loss 1.5472494013037685, accuracy 82.03125%\n",
      "step 284 loss 1.826567068361004, accuracy 78.90625%\n",
      "step 285 loss 1.913022479295866, accuracy 81.25%\n",
      "step 286 loss 1.9786479037035964, accuracy 75.78125%\n",
      "step 287 loss 1.5114427671452555, accuracy 86.71875%\n",
      "step 288 loss 1.4507478900822113, accuracy 92.1875%\n",
      "step 289 loss 1.7406771232410594, accuracy 85.15625%\n",
      "step 290 loss 1.7449456129166538, accuracy 81.25%\n",
      "step 291 loss 1.7342769719048132, accuracy 85.15625%\n",
      "step 292 loss 1.6759931517316633, accuracy 86.71875%\n",
      "step 293 loss 1.6969572075043111, accuracy 86.71875%\n",
      "step 294 loss 2.2511365792085996, accuracy 79.6875%\n",
      "step 295 loss 1.4517117946965896, accuracy 85.15625%\n",
      "step 296 loss 1.8734519102392384, accuracy 85.9375%\n",
      "step 297 loss 1.81321118475722, accuracy 82.8125%\n",
      "step 298 loss 1.3296593500056595, accuracy 87.5%\n",
      "step 299 loss 1.441553112202449, accuracy 91.40625%\n",
      "step 300 loss 1.5988842372828456, accuracy 82.03125%\n",
      "step 301 loss 1.4589305932119876, accuracy 87.5%\n",
      "step 302 loss 1.3160553761292457, accuracy 94.53125%\n",
      "step 303 loss 1.5202138960582894, accuracy 86.71875%\n",
      "step 304 loss 1.456744241743848, accuracy 88.28125%\n",
      "step 305 loss 1.4620813912759656, accuracy 87.5%\n",
      "step 306 loss 1.2965015477119268, accuracy 85.9375%\n",
      "step 307 loss 1.716139683193741, accuracy 85.15625%\n",
      "step 308 loss 1.8418767980137576, accuracy 89.0625%\n",
      "step 309 loss 1.63652867896731, accuracy 73.4375%\n",
      "step 310 loss 1.3528233424182339, accuracy 86.71875%\n",
      "step 311 loss 1.554656795485146, accuracy 80.46875%\n",
      "step 312 loss 1.4132459868121732, accuracy 89.0625%\n",
      "step 313 loss 1.1388455954737564, accuracy 90.625%\n",
      "step 314 loss 1.5211755989816407, accuracy 91.40625%\n",
      "step 315 loss 1.698452452783605, accuracy 84.375%\n",
      "step 316 loss 1.4590595946024156, accuracy 83.59375%\n",
      "step 317 loss 1.3215264757421603, accuracy 85.15625%\n",
      "step 318 loss 1.2221965858896613, accuracy 89.0625%\n",
      "step 319 loss 1.3952026195269553, accuracy 89.0625%\n",
      "step 320 loss 1.3609805031265139, accuracy 79.6875%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 321 loss 1.3274227231098636, accuracy 85.9375%\n",
      "step 322 loss 1.5689650789129967, accuracy 77.34375%\n",
      "step 323 loss 1.319739895589189, accuracy 89.0625%\n",
      "step 324 loss 2.1685022664879186, accuracy 71.09375%\n",
      "step 325 loss 1.9139379542067172, accuracy 80.46875%\n",
      "step 326 loss 1.603886113999393, accuracy 81.25%\n",
      "step 327 loss 1.1446826893362987, accuracy 88.28125%\n",
      "step 328 loss 1.8966468075367864, accuracy 80.46875%\n",
      "step 329 loss 1.8381610315986294, accuracy 82.03125%\n",
      "step 330 loss 1.361597356847657, accuracy 90.625%\n",
      "step 331 loss 1.5619178999702832, accuracy 88.28125%\n",
      "step 332 loss 1.9026697308756049, accuracy 87.5%\n",
      "step 333 loss 1.3372642492511422, accuracy 88.28125%\n",
      "step 334 loss 1.3062420364139902, accuracy 92.96875%\n",
      "step 335 loss 1.6236960751103116, accuracy 83.59375%\n",
      "step 336 loss 1.540730941633041, accuracy 83.59375%\n",
      "step 337 loss 1.372628348543106, accuracy 84.375%\n",
      "step 338 loss 1.68212438202574, accuracy 83.59375%\n",
      "step 339 loss 1.288919130948952, accuracy 91.40625%\n",
      "step 340 loss 1.5218519286279957, accuracy 89.84375%\n",
      "step 341 loss 1.101437308572089, accuracy 91.40625%\n",
      "step 342 loss 1.4786237427125382, accuracy 88.28125%\n",
      "step 343 loss 1.8860179645120256, accuracy 82.8125%\n",
      "step 344 loss 1.5110912238811691, accuracy 90.625%\n",
      "step 345 loss 1.758076150759071, accuracy 83.59375%\n",
      "step 346 loss 1.496466786875626, accuracy 85.15625%\n",
      "step 347 loss 1.2707036697018437, accuracy 87.5%\n",
      "step 348 loss 1.375109969049202, accuracy 86.71875%\n",
      "step 349 loss 1.5192344125281974, accuracy 92.1875%\n",
      "step 350 loss 1.3045522559989073, accuracy 94.53125%\n",
      "step 351 loss 1.9534853522633226, accuracy 88.28125%\n",
      "step 352 loss 1.7719511380013289, accuracy 81.25%\n",
      "step 353 loss 1.8309146931194356, accuracy 80.46875%\n",
      "step 354 loss 1.5511429703841677, accuracy 80.46875%\n",
      "step 355 loss 1.7224395259488956, accuracy 89.84375%\n",
      "step 356 loss 1.4235972888518493, accuracy 85.15625%\n",
      "step 357 loss 1.338534815448891, accuracy 90.625%\n",
      "step 358 loss 1.4718275704531198, accuracy 88.28125%\n",
      "step 359 loss 1.9736951171422248, accuracy 73.4375%\n",
      "step 360 loss 1.7549126919950715, accuracy 78.90625%\n",
      "step 361 loss 1.6462123937327207, accuracy 82.8125%\n",
      "step 362 loss 1.6078385966917434, accuracy 88.28125%\n",
      "step 363 loss 1.3834207404289682, accuracy 87.5%\n",
      "step 364 loss 1.4966960455014395, accuracy 86.71875%\n",
      "step 365 loss 1.1926867843406226, accuracy 88.28125%\n",
      "step 366 loss 1.2807231455270456, accuracy 89.84375%\n",
      "step 367 loss 1.152000051007406, accuracy 85.9375%\n",
      "step 368 loss 1.3271776878305939, accuracy 88.28125%\n",
      "step 369 loss 1.3397640006239127, accuracy 85.9375%\n",
      "step 370 loss 1.2348434192652527, accuracy 90.625%\n",
      "step 371 loss 1.3577420680451309, accuracy 87.5%\n",
      "step 372 loss 1.139862875043262, accuracy 93.75%\n",
      "step 373 loss 1.5694564860631632, accuracy 81.25%\n",
      "step 374 loss 1.3711094410703881, accuracy 91.40625%\n",
      "step 375 loss 2.0144250159276176, accuracy 75.78125%\n",
      "step 376 loss 1.3612740404538188, accuracy 86.71875%\n",
      "step 377 loss 1.2170111851853165, accuracy 85.15625%\n",
      "step 378 loss 1.1291575790523531, accuracy 91.40625%\n",
      "step 379 loss 1.2356580022688144, accuracy 94.53125%\n",
      "step 380 loss 1.4333629512939914, accuracy 93.75%\n",
      "step 381 loss 1.6079356158541789, accuracy 81.25%\n",
      "step 382 loss 1.2969156771329027, accuracy 87.5%\n",
      "step 383 loss 1.062382327837006, accuracy 88.28125%\n",
      "step 384 loss 1.7821797843831597, accuracy 83.59375%\n",
      "step 385 loss 1.2877871114984163, accuracy 91.40625%\n",
      "step 386 loss 1.5179851157393962, accuracy 88.28125%\n",
      "step 387 loss 2.122143346744273, accuracy 81.25%\n",
      "step 388 loss 1.6586277088170691, accuracy 87.5%\n",
      "step 389 loss 1.4100016826110668, accuracy 91.40625%\n",
      "step 390 loss 1.371364815517702, accuracy 89.0625%\n",
      "step 391 loss 1.4821990030451633, accuracy 84.375%\n",
      "step 392 loss 1.5271256938512623, accuracy 87.5%\n",
      "step 393 loss 1.621578427079868, accuracy 85.9375%\n",
      "step 394 loss 1.3422597848775124, accuracy 89.84375%\n",
      "step 395 loss 1.434569806196147, accuracy 89.0625%\n",
      "step 396 loss 1.5539441240283285, accuracy 87.5%\n",
      "step 397 loss 1.3862171700860388, accuracy 89.0625%\n",
      "step 398 loss 1.313744064289937, accuracy 90.625%\n",
      "step 399 loss 1.1889791325946173, accuracy 94.53125%\n",
      "step 400 loss 1.381234554657826, accuracy 90.625%\n",
      "step 401 loss 1.9388501389906552, accuracy 74.21875%\n",
      "step 402 loss 1.620727354529617, accuracy 85.15625%\n",
      "step 403 loss 1.3815934282629887, accuracy 88.28125%\n",
      "step 404 loss 1.7204022820905571, accuracy 82.8125%\n",
      "step 405 loss 1.3039972501114052, accuracy 91.40625%\n",
      "step 406 loss 1.8228779212103683, accuracy 83.59375%\n",
      "step 407 loss 1.7591855531280445, accuracy 84.375%\n",
      "step 408 loss 1.1873322423257116, accuracy 90.625%\n",
      "step 409 loss 1.4288309407741364, accuracy 89.84375%\n",
      "step 410 loss 1.4938299113289377, accuracy 83.59375%\n",
      "step 411 loss 1.5944617510985932, accuracy 85.9375%\n",
      "step 412 loss 1.233293951717512, accuracy 89.84375%\n",
      "step 413 loss 1.5227699243795871, accuracy 85.15625%\n",
      "step 414 loss 1.5554855001739551, accuracy 84.375%\n",
      "step 415 loss 1.0684065357007713, accuracy 91.40625%\n",
      "step 416 loss 1.0274516420194317, accuracy 89.84375%\n",
      "step 417 loss 0.9000610458875392, accuracy 95.3125%\n",
      "step 418 loss 1.3817401853650229, accuracy 87.5%\n",
      "step 419 loss 1.0620856056677361, accuracy 90.625%\n",
      "step 420 loss 1.139306083526061, accuracy 96.09375%\n",
      "step 421 loss 1.3514785252544412, accuracy 87.5%\n",
      "step 422 loss 1.50774841070919, accuracy 83.59375%\n",
      "step 423 loss 1.4595718893544534, accuracy 87.5%\n",
      "step 424 loss 1.5082747308014923, accuracy 84.375%\n",
      "step 425 loss 0.9447236476206545, accuracy 93.75%\n",
      "step 426 loss 1.0989969219872056, accuracy 92.1875%\n",
      "step 427 loss 1.7392096279970313, accuracy 79.6875%\n",
      "step 428 loss 1.1384653486931862, accuracy 92.1875%\n",
      "step 429 loss 1.1472289622726444, accuracy 92.1875%\n",
      "step 430 loss 1.2693123036176674, accuracy 90.625%\n",
      "step 431 loss 1.4010790862386522, accuracy 85.9375%\n",
      "step 432 loss 1.4763585744322816, accuracy 90.625%\n",
      "step 433 loss 1.7352928179360936, accuracy 89.0625%\n",
      "step 434 loss 1.1830901091824395, accuracy 89.84375%\n",
      "step 435 loss 1.0797128694203229, accuracy 94.53125%\n",
      "step 436 loss 1.0941787174745934, accuracy 92.1875%\n",
      "step 437 loss 0.935537789126436, accuracy 93.75%\n",
      "step 438 loss 1.1409579890324149, accuracy 92.1875%\n",
      "step 439 loss 1.180990623585429, accuracy 90.625%\n",
      "step 440 loss 1.5520858983097643, accuracy 86.71875%\n",
      "step 441 loss 1.2449955128634742, accuracy 85.9375%\n",
      "step 442 loss 1.3884806160024554, accuracy 87.5%\n",
      "step 443 loss 1.3164067772937744, accuracy 89.84375%\n",
      "step 444 loss 0.9590275434783091, accuracy 90.625%\n",
      "step 445 loss 2.1357943847989227, accuracy 85.15625%\n",
      "step 446 loss 1.2062733680653062, accuracy 90.625%\n",
      "step 447 loss 1.3694684516599358, accuracy 86.71875%\n",
      "step 448 loss 1.2881571426631302, accuracy 85.9375%\n",
      "step 449 loss 1.086958343708033, accuracy 86.71875%\n",
      "step 450 loss 1.4766661594184822, accuracy 82.8125%\n",
      "step 451 loss 1.3937015025066817, accuracy 89.0625%\n",
      "step 452 loss 1.2189440542529133, accuracy 85.9375%\n",
      "step 453 loss 1.2160862276446331, accuracy 85.15625%\n",
      "step 454 loss 1.0379525702002999, accuracy 92.96875%\n",
      "step 455 loss 1.1731406978451966, accuracy 85.15625%\n",
      "step 456 loss 0.9831993864638593, accuracy 88.28125%\n",
      "step 457 loss 1.198728071130976, accuracy 90.625%\n",
      "step 458 loss 1.343057449918153, accuracy 90.625%\n",
      "step 459 loss 1.6213207581069673, accuracy 92.96875%\n",
      "step 460 loss 0.9449125003774257, accuracy 93.75%\n",
      "step 461 loss 1.3959123446051926, accuracy 90.625%\n",
      "step 462 loss 1.0341274213371467, accuracy 90.625%\n",
      "step 463 loss 1.3263758071128608, accuracy 89.0625%\n",
      "step 464 loss 0.9680175620813675, accuracy 92.96875%\n",
      "step 465 loss 0.9482813464179041, accuracy 90.625%\n",
      "step 466 loss 1.4198865820512006, accuracy 92.96875%\n",
      "step 467 loss 1.2782585923380974, accuracy 89.84375%\n",
      "step 468 loss 1.15287320787957, accuracy 91.40625%\n",
      "step 469 loss 1.1697350713266708, accuracy 89.0625%\n",
      "step 470 loss 0.8270496495898347, accuracy 96.875%\n",
      "step 471 loss 1.0223011255158059, accuracy 92.96875%\n",
      "step 472 loss 1.6107954336358186, accuracy 81.25%\n",
      "step 473 loss 1.5733427242464673, accuracy 90.625%\n",
      "step 474 loss 1.145522246534401, accuracy 92.96875%\n",
      "step 475 loss 1.2537764133355447, accuracy 87.5%\n",
      "step 476 loss 1.2865864879197155, accuracy 92.96875%\n",
      "step 477 loss 0.9841837592868098, accuracy 91.40625%\n",
      "step 478 loss 1.6602245031479428, accuracy 90.625%\n",
      "step 479 loss 1.2633363327869156, accuracy 90.625%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 480 loss 1.4249955974539512, accuracy 92.96875%\n",
      "step 481 loss 1.4983799076183435, accuracy 91.40625%\n",
      "step 482 loss 1.2724925822466113, accuracy 89.84375%\n",
      "step 483 loss 1.4355395831335636, accuracy 91.40625%\n",
      "step 484 loss 1.2865087311224839, accuracy 89.84375%\n",
      "step 485 loss 1.3652772298033988, accuracy 84.375%\n",
      "step 486 loss 1.1139614125220954, accuracy 90.625%\n",
      "step 487 loss 1.120861653218496, accuracy 92.1875%\n",
      "step 488 loss 0.8173142420244355, accuracy 95.3125%\n",
      "step 489 loss 0.8916620499067851, accuracy 92.1875%\n",
      "step 490 loss 1.2696667856776844, accuracy 92.96875%\n",
      "step 491 loss 1.1251602149848396, accuracy 90.625%\n",
      "step 492 loss 1.1010300998626423, accuracy 91.40625%\n",
      "step 493 loss 1.2713533464244027, accuracy 91.40625%\n",
      "step 494 loss 1.010548791773961, accuracy 92.1875%\n",
      "step 495 loss 1.0420128264495103, accuracy 89.84375%\n",
      "step 496 loss 1.1603413689262028, accuracy 91.40625%\n",
      "step 497 loss 1.3617732172851793, accuracy 93.75%\n",
      "step 498 loss 1.2095392299092746, accuracy 92.1875%\n",
      "step 499 loss 1.1356055288986229, accuracy 91.40625%\n",
      "step 500 loss 1.22596024643205, accuracy 92.96875%\n",
      "step 501 loss 1.530031251672542, accuracy 81.25%\n",
      "step 502 loss 0.9793788007387161, accuracy 92.1875%\n",
      "step 503 loss 1.2221332281784063, accuracy 90.625%\n",
      "step 504 loss 1.5700165153451437, accuracy 87.5%\n",
      "step 505 loss 1.337998059128597, accuracy 89.0625%\n",
      "step 506 loss 1.0741570214462401, accuracy 96.09375%\n",
      "step 507 loss 0.9826973686042733, accuracy 94.53125%\n",
      "step 508 loss 1.2523440708684939, accuracy 91.40625%\n",
      "step 509 loss 1.4582191227467876, accuracy 85.15625%\n",
      "step 510 loss 1.324106482061052, accuracy 92.1875%\n",
      "step 511 loss 0.9537558941433462, accuracy 94.53125%\n",
      "step 512 loss 1.7576134658104379, accuracy 92.1875%\n",
      "step 513 loss 1.0295814400633065, accuracy 94.53125%\n",
      "step 514 loss 1.2616779618517393, accuracy 92.1875%\n",
      "step 515 loss 1.2483755100238416, accuracy 92.1875%\n",
      "step 516 loss 0.992141814142432, accuracy 93.75%\n",
      "step 517 loss 1.440686057706306, accuracy 89.0625%\n",
      "step 518 loss 1.341001779665714, accuracy 90.625%\n",
      "step 519 loss 1.1185903176747047, accuracy 93.75%\n",
      "step 520 loss 1.3298828500768312, accuracy 92.1875%\n",
      "step 521 loss 1.131085612291553, accuracy 92.1875%\n",
      "step 522 loss 1.071878212397736, accuracy 92.1875%\n",
      "step 523 loss 1.081508294096041, accuracy 87.5%\n",
      "step 524 loss 0.964139817877114, accuracy 92.1875%\n",
      "step 525 loss 0.875829430573719, accuracy 95.3125%\n",
      "step 526 loss 1.2865687624890807, accuracy 96.875%\n",
      "step 527 loss 0.9620449230312978, accuracy 92.96875%\n",
      "step 528 loss 1.042894942062528, accuracy 90.625%\n",
      "step 529 loss 1.3453950479086179, accuracy 91.40625%\n",
      "step 530 loss 0.9859495941790181, accuracy 94.53125%\n",
      "step 531 loss 1.1086276382689921, accuracy 87.5%\n",
      "step 532 loss 0.9158426271109226, accuracy 94.53125%\n",
      "step 533 loss 1.66836787926719, accuracy 85.15625%\n",
      "step 534 loss 0.9658853951582691, accuracy 92.1875%\n",
      "step 535 loss 0.9980924828036918, accuracy 96.875%\n",
      "step 536 loss 0.7083000531692563, accuracy 94.53125%\n",
      "step 537 loss 1.2732823263285338, accuracy 92.1875%\n",
      "step 538 loss 1.562705264989909, accuracy 85.9375%\n",
      "step 539 loss 1.3915601396746065, accuracy 86.71875%\n",
      "step 540 loss 1.4349749911572025, accuracy 86.71875%\n",
      "step 541 loss 1.466983123607073, accuracy 89.84375%\n",
      "step 542 loss 1.2771640842984084, accuracy 87.5%\n",
      "step 543 loss 1.3535059190713479, accuracy 89.0625%\n",
      "step 544 loss 0.836845345964143, accuracy 92.1875%\n",
      "step 545 loss 1.125404213151204, accuracy 91.40625%\n",
      "step 546 loss 1.0265666878861455, accuracy 92.1875%\n",
      "step 547 loss 1.1653543088821456, accuracy 95.3125%\n",
      "step 548 loss 1.1253038069363668, accuracy 94.53125%\n",
      "step 549 loss 0.8359656288250733, accuracy 96.09375%\n",
      "step 550 loss 1.0195689191048936, accuracy 91.40625%\n",
      "step 551 loss 0.900117081680107, accuracy 93.75%\n",
      "step 552 loss 1.0052504914004412, accuracy 92.96875%\n",
      "step 553 loss 1.421627844439613, accuracy 90.625%\n",
      "step 554 loss 1.0130217656410547, accuracy 94.53125%\n",
      "step 555 loss 1.2872461014035121, accuracy 92.96875%\n",
      "step 556 loss 0.8627505351479682, accuracy 93.75%\n",
      "step 557 loss 0.9223522891553056, accuracy 90.625%\n",
      "step 558 loss 1.0635853223186365, accuracy 95.3125%\n",
      "step 559 loss 1.3718038864531177, accuracy 93.75%\n",
      "step 560 loss 0.8459047534515899, accuracy 95.3125%\n",
      "step 561 loss 0.9718707436885555, accuracy 91.40625%\n",
      "step 562 loss 1.0745256796405631, accuracy 92.96875%\n",
      "step 563 loss 1.0280931893998857, accuracy 90.625%\n",
      "step 564 loss 0.8514525027516969, accuracy 92.96875%\n",
      "step 565 loss 1.084790686819414, accuracy 94.53125%\n",
      "step 566 loss 1.3837422314323857, accuracy 89.84375%\n",
      "step 567 loss 1.035922794344292, accuracy 91.40625%\n",
      "step 568 loss 1.039531718975836, accuracy 90.625%\n",
      "step 569 loss 1.0776899966124893, accuracy 92.96875%\n",
      "step 570 loss 0.8558808420592682, accuracy 97.65625%\n",
      "step 571 loss 1.2102861032968175, accuracy 91.40625%\n",
      "step 572 loss 1.0499099982270517, accuracy 92.96875%\n",
      "step 573 loss 1.068957941914244, accuracy 94.53125%\n",
      "step 574 loss 1.1671944985517122, accuracy 92.96875%\n",
      "step 575 loss 0.8898709449371738, accuracy 92.96875%\n",
      "step 576 loss 1.3029032304739419, accuracy 92.1875%\n",
      "step 577 loss 1.0368911091670665, accuracy 91.40625%\n",
      "step 578 loss 1.0591375503385922, accuracy 92.1875%\n",
      "step 579 loss 0.7910228879208426, accuracy 94.53125%\n",
      "step 580 loss 0.8832528892607788, accuracy 93.75%\n",
      "step 581 loss 0.9676241602093356, accuracy 95.3125%\n",
      "step 582 loss 1.2558436151146055, accuracy 93.75%\n",
      "step 583 loss 0.9707863162125207, accuracy 93.75%\n",
      "step 584 loss 0.8707245745479847, accuracy 91.40625%\n",
      "step 585 loss 1.0760755868291518, accuracy 93.75%\n",
      "step 586 loss 1.018981150832314, accuracy 90.625%\n",
      "step 587 loss 1.018784228687846, accuracy 90.625%\n",
      "step 588 loss 0.8688732564415246, accuracy 95.3125%\n",
      "step 589 loss 0.9986332571104675, accuracy 94.53125%\n",
      "step 590 loss 1.1551324929382387, accuracy 94.53125%\n",
      "step 591 loss 1.0250475172383628, accuracy 96.875%\n",
      "step 592 loss 0.9409547028226868, accuracy 93.75%\n",
      "step 593 loss 0.9779331860655103, accuracy 96.875%\n",
      "step 594 loss 0.9261322473347708, accuracy 89.84375%\n",
      "step 595 loss 1.0226797404391093, accuracy 89.84375%\n",
      "step 596 loss 0.8810455127546111, accuracy 93.75%\n",
      "step 597 loss 1.1230410395589794, accuracy 91.40625%\n",
      "step 598 loss 0.8900725859957983, accuracy 96.09375%\n",
      "step 599 loss 0.8404572965047574, accuracy 94.53125%\n",
      "step 600 loss 1.2749043831018712, accuracy 90.625%\n",
      "step 601 loss 1.0002873957405436, accuracy 93.75%\n",
      "step 602 loss 0.9502271525097338, accuracy 92.96875%\n",
      "step 603 loss 0.8738444900936736, accuracy 93.75%\n",
      "step 604 loss 1.248394688985993, accuracy 93.75%\n",
      "step 605 loss 1.1300190174876306, accuracy 92.96875%\n",
      "step 606 loss 0.9138692000289731, accuracy 92.96875%\n",
      "step 607 loss 0.9528824972108634, accuracy 92.96875%\n",
      "step 608 loss 1.1142818486190385, accuracy 91.40625%\n",
      "step 609 loss 0.867295329598724, accuracy 92.1875%\n",
      "step 610 loss 0.6961474268349049, accuracy 96.875%\n",
      "step 611 loss 1.062597762916062, accuracy 93.75%\n",
      "step 612 loss 1.237196513649954, accuracy 90.625%\n",
      "step 613 loss 1.2680371166291937, accuracy 87.5%\n",
      "step 614 loss 1.117815974932327, accuracy 91.40625%\n",
      "step 615 loss 0.973090049431302, accuracy 94.53125%\n",
      "step 616 loss 1.0257563523728397, accuracy 90.625%\n",
      "step 617 loss 0.767680500794661, accuracy 92.96875%\n",
      "step 618 loss 1.0148726115635727, accuracy 93.75%\n",
      "step 619 loss 0.9902782979336476, accuracy 93.75%\n",
      "step 620 loss 0.8244599275371551, accuracy 95.3125%\n",
      "step 621 loss 1.2947747167601664, accuracy 89.84375%\n",
      "step 622 loss 0.8781238485532555, accuracy 93.75%\n",
      "step 623 loss 1.6313721968132113, accuracy 92.1875%\n",
      "step 624 loss 0.9531938464157919, accuracy 93.75%\n",
      "step 625 loss 0.6113465213687825, accuracy 96.875%\n",
      "step 626 loss 0.9328395913724853, accuracy 92.96875%\n",
      "step 627 loss 0.8035073590536603, accuracy 96.09375%\n",
      "step 628 loss 1.0926485270459052, accuracy 93.75%\n",
      "step 629 loss 0.952182433020829, accuracy 96.09375%\n",
      "step 630 loss 0.9706956228887049, accuracy 92.96875%\n",
      "step 631 loss 1.3676419541778526, accuracy 96.09375%\n",
      "step 632 loss 0.8510732940437984, accuracy 92.1875%\n",
      "step 633 loss 1.1304335139146398, accuracy 90.625%\n",
      "step 634 loss 0.9934428258692718, accuracy 94.53125%\n",
      "step 635 loss 1.1456325960801246, accuracy 91.40625%\n",
      "step 636 loss 0.9983547780841894, accuracy 95.3125%\n",
      "step 637 loss 1.1324570996472318, accuracy 88.28125%\n",
      "step 638 loss 1.0663004826040836, accuracy 88.28125%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 639 loss 1.0912831260007954, accuracy 85.9375%\n",
      "step 640 loss 0.8845274222391188, accuracy 92.1875%\n",
      "step 641 loss 0.9015016597484352, accuracy 91.40625%\n",
      "step 642 loss 0.8929357683089868, accuracy 89.84375%\n",
      "step 643 loss 0.8565635518122442, accuracy 94.53125%\n",
      "step 644 loss 0.9228364503475575, accuracy 93.75%\n",
      "step 645 loss 0.8902096339388743, accuracy 91.40625%\n",
      "step 646 loss 1.0298979806356814, accuracy 92.96875%\n",
      "step 647 loss 1.214635599739288, accuracy 94.53125%\n",
      "step 648 loss 0.9556022896273436, accuracy 91.40625%\n",
      "step 649 loss 1.1103138702524593, accuracy 93.75%\n",
      "step 650 loss 0.924563482684136, accuracy 91.40625%\n",
      "step 651 loss 1.1613921560564833, accuracy 91.40625%\n",
      "step 652 loss 1.071450920611189, accuracy 91.40625%\n",
      "step 653 loss 0.905630699710606, accuracy 89.84375%\n",
      "step 654 loss 1.2631464693008334, accuracy 91.40625%\n",
      "step 655 loss 0.8250037310701367, accuracy 93.75%\n",
      "step 656 loss 0.8214013315493914, accuracy 95.3125%\n",
      "step 657 loss 0.7517068466889684, accuracy 96.09375%\n",
      "step 658 loss 0.8024027099417417, accuracy 92.96875%\n",
      "step 659 loss 0.890264819611074, accuracy 92.1875%\n",
      "step 660 loss 0.8430645460162081, accuracy 95.3125%\n",
      "step 661 loss 0.9127024905510315, accuracy 96.09375%\n",
      "step 662 loss 1.1820651411095373, accuracy 90.625%\n",
      "step 663 loss 0.9011770150185249, accuracy 88.28125%\n",
      "step 664 loss 0.982976214300302, accuracy 89.84375%\n",
      "step 665 loss 1.0595352036100811, accuracy 94.53125%\n",
      "step 666 loss 0.8593492845585081, accuracy 96.875%\n",
      "step 667 loss 0.9546559113905208, accuracy 90.625%\n",
      "step 668 loss 0.715003421080161, accuracy 96.09375%\n",
      "step 669 loss 0.7856059187082101, accuracy 96.875%\n",
      "step 670 loss 0.8787572624942783, accuracy 91.40625%\n",
      "step 671 loss 0.7939073658844388, accuracy 92.96875%\n",
      "step 672 loss 0.8837285595703201, accuracy 96.09375%\n",
      "step 673 loss 0.8958081805564957, accuracy 92.96875%\n",
      "step 674 loss 0.9859914229048522, accuracy 95.3125%\n",
      "step 675 loss 0.7425205047963183, accuracy 92.96875%\n",
      "step 676 loss 1.1519319930678416, accuracy 92.1875%\n",
      "step 677 loss 0.8380856852588211, accuracy 94.53125%\n",
      "step 678 loss 1.1216651087533083, accuracy 89.84375%\n",
      "step 679 loss 0.8384844531562968, accuracy 92.96875%\n",
      "step 680 loss 0.8610336722130717, accuracy 92.1875%\n",
      "step 681 loss 0.9017289481803042, accuracy 93.75%\n",
      "step 682 loss 0.8805238594009746, accuracy 92.1875%\n",
      "step 683 loss 0.9453340204230919, accuracy 94.53125%\n",
      "step 684 loss 0.8977776672487553, accuracy 92.96875%\n",
      "step 685 loss 1.0600927143572878, accuracy 92.1875%\n",
      "step 686 loss 0.5944090374689697, accuracy 96.875%\n",
      "step 687 loss 0.7254234685554849, accuracy 95.3125%\n",
      "step 688 loss 0.9767392913660033, accuracy 91.40625%\n",
      "step 689 loss 1.3019000956701745, accuracy 92.96875%\n",
      "step 690 loss 0.7805985749255531, accuracy 94.53125%\n",
      "step 691 loss 0.7600712399782118, accuracy 97.65625%\n",
      "step 692 loss 0.9579910738208866, accuracy 95.3125%\n",
      "step 693 loss 0.8767692018644575, accuracy 92.96875%\n",
      "step 694 loss 1.0138328890215695, accuracy 91.40625%\n",
      "step 695 loss 1.1279329783048788, accuracy 89.84375%\n",
      "step 696 loss 0.7953423368621685, accuracy 96.875%\n",
      "step 697 loss 1.022860206277514, accuracy 94.53125%\n",
      "step 698 loss 1.1512047187907033, accuracy 94.53125%\n",
      "step 699 loss 0.617777227054707, accuracy 94.53125%\n",
      "step 700 loss 1.3477012778558173, accuracy 88.28125%\n",
      "step 701 loss 1.1093008183186912, accuracy 92.1875%\n",
      "step 702 loss 0.7761243397048321, accuracy 95.3125%\n",
      "step 703 loss 1.110936587582749, accuracy 95.3125%\n",
      "step 704 loss 1.1322645315776276, accuracy 92.96875%\n",
      "step 705 loss 1.0916821553677933, accuracy 93.75%\n",
      "step 706 loss 1.1145561715339556, accuracy 92.96875%\n",
      "step 707 loss 0.8758385731395875, accuracy 93.75%\n",
      "step 708 loss 1.0138087891945213, accuracy 95.3125%\n",
      "step 709 loss 0.9219190840592715, accuracy 92.1875%\n",
      "step 710 loss 1.1917850297104544, accuracy 92.96875%\n",
      "step 711 loss 0.8452950598917132, accuracy 97.65625%\n",
      "step 712 loss 0.6846237277869053, accuracy 98.4375%\n",
      "step 713 loss 0.9128145369757206, accuracy 92.96875%\n",
      "step 714 loss 1.122406349667157, accuracy 90.625%\n",
      "step 715 loss 0.9249445660460792, accuracy 91.40625%\n",
      "step 716 loss 0.9759309101397424, accuracy 92.96875%\n",
      "step 717 loss 0.7832629322135843, accuracy 95.3125%\n",
      "step 718 loss 0.8348569862829718, accuracy 92.96875%\n",
      "step 719 loss 1.0024766491385224, accuracy 95.3125%\n",
      "step 720 loss 0.9988088314121465, accuracy 91.40625%\n",
      "step 721 loss 0.9948367853826175, accuracy 91.40625%\n",
      "step 722 loss 0.9697172286236436, accuracy 95.3125%\n",
      "step 723 loss 0.931134435155844, accuracy 92.96875%\n",
      "step 724 loss 0.8014524498173163, accuracy 97.65625%\n",
      "step 725 loss 0.958816537806396, accuracy 94.53125%\n",
      "step 726 loss 0.6503206792163613, accuracy 96.09375%\n",
      "step 727 loss 1.0877400756183466, accuracy 91.40625%\n",
      "step 728 loss 0.9984179514713094, accuracy 92.96875%\n",
      "step 729 loss 0.7769279713726477, accuracy 96.09375%\n",
      "step 730 loss 0.7438146470219653, accuracy 97.65625%\n",
      "step 731 loss 0.8776409281105902, accuracy 92.96875%\n",
      "step 732 loss 0.8023665853794875, accuracy 95.3125%\n",
      "step 733 loss 0.8688930359825022, accuracy 92.96875%\n",
      "step 734 loss 0.8467564752937079, accuracy 95.3125%\n",
      "step 735 loss 0.8273075848508075, accuracy 93.75%\n",
      "step 736 loss 0.7742363436884967, accuracy 95.3125%\n",
      "step 737 loss 0.7723297156797552, accuracy 94.53125%\n",
      "step 738 loss 0.9104415503015367, accuracy 93.75%\n",
      "step 739 loss 1.1073836569140791, accuracy 93.75%\n",
      "step 740 loss 0.7615445399425042, accuracy 95.3125%\n",
      "step 741 loss 0.9637996927774984, accuracy 95.3125%\n",
      "step 742 loss 0.8177648271467624, accuracy 96.875%\n",
      "step 743 loss 0.9405110110064994, accuracy 92.96875%\n",
      "step 744 loss 0.6434294412065251, accuracy 96.09375%\n",
      "step 745 loss 0.858256822648255, accuracy 94.53125%\n",
      "step 746 loss 0.5842958545825114, accuracy 96.875%\n",
      "step 747 loss 1.0396499544090696, accuracy 92.96875%\n",
      "step 748 loss 1.1209488498288827, accuracy 92.96875%\n",
      "step 749 loss 0.862127046002932, accuracy 92.96875%\n",
      "step 750 loss 0.9460422025795105, accuracy 96.875%\n",
      "step 751 loss 0.9457642067966476, accuracy 96.09375%\n",
      "step 752 loss 0.8783526712181043, accuracy 93.75%\n",
      "step 753 loss 0.7173973966001878, accuracy 93.75%\n",
      "step 754 loss 1.1271352708766005, accuracy 92.1875%\n",
      "step 755 loss 1.4538858043165042, accuracy 92.1875%\n",
      "step 756 loss 0.7207591681442684, accuracy 96.875%\n",
      "step 757 loss 1.0812883548378496, accuracy 93.75%\n",
      "step 758 loss 0.9035795493900405, accuracy 92.1875%\n",
      "step 759 loss 0.9851156264401679, accuracy 92.96875%\n",
      "step 760 loss 0.6827509782883167, accuracy 96.09375%\n",
      "step 761 loss 0.7125453033574484, accuracy 95.3125%\n",
      "step 762 loss 0.8131042569366949, accuracy 95.3125%\n",
      "step 763 loss 0.8075547432174316, accuracy 95.3125%\n",
      "step 764 loss 0.9756734896619399, accuracy 92.96875%\n",
      "step 765 loss 0.9308005969334928, accuracy 92.1875%\n",
      "step 766 loss 0.7914278093311966, accuracy 95.3125%\n",
      "step 767 loss 0.719011820658703, accuracy 96.875%\n",
      "step 768 loss 0.7514001094220114, accuracy 94.53125%\n",
      "step 769 loss 0.6877289066107316, accuracy 94.53125%\n",
      "step 770 loss 1.0020450923804876, accuracy 96.09375%\n",
      "step 771 loss 0.9853624712314898, accuracy 94.53125%\n",
      "step 772 loss 0.8262909646723579, accuracy 93.75%\n",
      "step 773 loss 0.9664517666598883, accuracy 92.1875%\n",
      "step 774 loss 0.8338170142786576, accuracy 96.875%\n",
      "step 775 loss 0.7666523705730276, accuracy 96.09375%\n",
      "step 776 loss 0.9136788547574471, accuracy 92.96875%\n",
      "step 777 loss 1.0024655305299746, accuracy 95.3125%\n",
      "step 778 loss 0.8618006776688795, accuracy 94.53125%\n",
      "step 779 loss 0.9197232242928913, accuracy 94.53125%\n",
      "step 780 loss 0.7972901182458993, accuracy 96.875%\n",
      "step 781 loss 0.6648272162968392, accuracy 94.53125%\n",
      "step 782 loss 0.8496311817542548, accuracy 93.75%\n",
      "step 783 loss 0.763815156058611, accuracy 94.53125%\n",
      "step 784 loss 0.8980894324070443, accuracy 96.09375%\n",
      "step 785 loss 0.9420205974366066, accuracy 95.3125%\n",
      "step 786 loss 0.5891015319151641, accuracy 97.65625%\n",
      "step 787 loss 0.7794932988086525, accuracy 98.4375%\n",
      "step 788 loss 0.7889875606521795, accuracy 97.65625%\n",
      "step 789 loss 0.7877352425800442, accuracy 92.96875%\n",
      "step 790 loss 0.6868048707208106, accuracy 97.65625%\n",
      "step 791 loss 1.0105681680072427, accuracy 96.09375%\n",
      "step 792 loss 1.5261070017299945, accuracy 91.40625%\n",
      "step 793 loss 0.7735735730178966, accuracy 95.3125%\n",
      "step 794 loss 0.7556480474026721, accuracy 94.53125%\n",
      "step 795 loss 0.689864022719557, accuracy 94.53125%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 796 loss 1.3433942999672297, accuracy 93.75%\n",
      "step 797 loss 0.7530835075477944, accuracy 94.53125%\n",
      "step 798 loss 0.8161709218979117, accuracy 93.75%\n",
      "step 799 loss 0.9347320747442456, accuracy 92.96875%\n",
      "step 800 loss 0.7213828644371458, accuracy 95.3125%\n",
      "step 801 loss 1.0363707516260259, accuracy 93.75%\n",
      "step 802 loss 0.8646897858716381, accuracy 94.53125%\n",
      "step 803 loss 0.9380643860055061, accuracy 94.53125%\n",
      "step 804 loss 1.117420239266518, accuracy 92.96875%\n",
      "step 805 loss 0.9261318151891413, accuracy 92.1875%\n",
      "step 806 loss 0.8112583634345102, accuracy 92.96875%\n",
      "step 807 loss 0.6292417982206188, accuracy 97.65625%\n",
      "step 808 loss 0.9765245143113966, accuracy 93.75%\n",
      "step 809 loss 0.7201725245197792, accuracy 96.09375%\n",
      "step 810 loss 1.0191231483764922, accuracy 94.53125%\n",
      "step 811 loss 0.7639456149953161, accuracy 92.96875%\n",
      "step 812 loss 0.6630475272130633, accuracy 96.09375%\n",
      "step 813 loss 1.0109810933115588, accuracy 96.09375%\n",
      "step 814 loss 0.8331888853352659, accuracy 90.625%\n",
      "step 815 loss 0.654887005316851, accuracy 95.3125%\n",
      "step 816 loss 0.709402366119115, accuracy 96.09375%\n",
      "step 817 loss 0.9994306812912729, accuracy 94.53125%\n",
      "step 818 loss 0.9321786535203734, accuracy 91.40625%\n",
      "step 819 loss 0.621202295214131, accuracy 93.75%\n",
      "step 820 loss 0.7061451245275806, accuracy 96.09375%\n",
      "step 821 loss 1.0685046905692628, accuracy 92.1875%\n",
      "step 822 loss 0.9042487525194701, accuracy 94.53125%\n",
      "step 823 loss 1.1077458499160655, accuracy 93.75%\n",
      "step 824 loss 0.7418973327659112, accuracy 95.3125%\n",
      "step 825 loss 1.0762807882870369, accuracy 90.625%\n",
      "step 826 loss 1.0293650098324543, accuracy 89.84375%\n",
      "step 827 loss 0.9757179502705419, accuracy 92.1875%\n",
      "step 828 loss 0.6696408094075907, accuracy 96.09375%\n",
      "step 829 loss 0.869172822167946, accuracy 92.1875%\n",
      "step 830 loss 0.9285843717644625, accuracy 91.40625%\n",
      "step 831 loss 1.046070282453369, accuracy 93.75%\n",
      "step 832 loss 0.8386571007643522, accuracy 91.40625%\n",
      "step 833 loss 0.8022804522030557, accuracy 94.53125%\n",
      "step 834 loss 0.6614834699811364, accuracy 98.4375%\n",
      "step 835 loss 0.6044435853370991, accuracy 95.3125%\n",
      "step 836 loss 0.7772350284396276, accuracy 94.53125%\n",
      "step 837 loss 0.8712947618531957, accuracy 94.53125%\n",
      "step 838 loss 0.9408079803156554, accuracy 96.09375%\n",
      "step 839 loss 0.724960977210521, accuracy 98.4375%\n",
      "step 840 loss 0.8166521208729075, accuracy 95.3125%\n",
      "step 841 loss 0.5421624765153669, accuracy 96.875%\n",
      "step 842 loss 0.7081945767613848, accuracy 98.4375%\n",
      "step 843 loss 0.6406718069111029, accuracy 96.09375%\n",
      "step 844 loss 0.5126884934776123, accuracy 97.65625%\n",
      "step 845 loss 0.9131967140565898, accuracy 94.53125%\n",
      "step 846 loss 0.9546159845147574, accuracy 95.3125%\n",
      "step 847 loss 0.8085735868362041, accuracy 94.53125%\n",
      "step 848 loss 1.0226326209766834, accuracy 94.53125%\n",
      "step 849 loss 0.7950629074119363, accuracy 94.53125%\n",
      "step 850 loss 0.6312907376666133, accuracy 96.875%\n",
      "step 851 loss 0.753139874635066, accuracy 92.96875%\n",
      "step 852 loss 0.7122457668199037, accuracy 95.3125%\n",
      "step 853 loss 0.8438315627504487, accuracy 94.53125%\n",
      "step 854 loss 0.7059040620117709, accuracy 96.09375%\n",
      "step 855 loss 1.0638490579586122, accuracy 90.625%\n",
      "step 856 loss 0.7148441820202226, accuracy 94.53125%\n",
      "step 857 loss 0.8507796382151558, accuracy 91.40625%\n",
      "step 858 loss 0.9295379292932242, accuracy 92.1875%\n",
      "step 859 loss 0.7628450334010934, accuracy 97.65625%\n",
      "step 860 loss 0.9135098240852152, accuracy 92.96875%\n",
      "step 861 loss 0.8784866965613014, accuracy 89.0625%\n",
      "step 862 loss 1.0147445367922732, accuracy 91.40625%\n",
      "step 863 loss 0.7944885010453847, accuracy 93.75%\n",
      "step 864 loss 0.6701472070468636, accuracy 98.4375%\n",
      "step 865 loss 1.1490430296253307, accuracy 92.1875%\n",
      "step 866 loss 0.5953513372428059, accuracy 96.875%\n",
      "step 867 loss 0.6228958739967008, accuracy 97.65625%\n",
      "step 868 loss 0.9540198201824378, accuracy 97.65625%\n",
      "step 869 loss 0.8004147202056394, accuracy 97.65625%\n",
      "step 870 loss 0.7331843232900521, accuracy 96.09375%\n",
      "step 871 loss 0.5560108131404428, accuracy 96.875%\n",
      "step 872 loss 0.7475220750227166, accuracy 96.875%\n",
      "step 873 loss 0.903471842324666, accuracy 93.75%\n",
      "step 874 loss 0.6703619059416851, accuracy 96.09375%\n",
      "step 875 loss 1.1578975869172041, accuracy 92.96875%\n",
      "step 876 loss 0.8723385941442546, accuracy 94.53125%\n",
      "step 877 loss 0.8437704290262978, accuracy 96.09375%\n",
      "step 878 loss 0.6081237464642538, accuracy 97.65625%\n",
      "step 879 loss 0.7428220184795509, accuracy 96.875%\n",
      "step 880 loss 0.8389008160566, accuracy 93.75%\n",
      "step 881 loss 0.5834336941073587, accuracy 96.09375%\n",
      "step 882 loss 0.7001519540825872, accuracy 93.75%\n",
      "step 883 loss 0.6991964737719211, accuracy 94.53125%\n",
      "step 884 loss 0.6904775804650665, accuracy 92.96875%\n",
      "step 885 loss 1.0748637516028632, accuracy 89.84375%\n",
      "step 886 loss 0.846076775888623, accuracy 92.1875%\n",
      "step 887 loss 0.952259446691852, accuracy 90.625%\n",
      "step 888 loss 0.8131631005633506, accuracy 93.75%\n",
      "step 889 loss 0.8339657184385052, accuracy 92.96875%\n",
      "step 890 loss 0.9513726492844308, accuracy 92.96875%\n",
      "step 891 loss 0.7436288009951978, accuracy 92.1875%\n",
      "step 892 loss 0.7048813518527614, accuracy 91.40625%\n",
      "step 893 loss 0.6431165174971588, accuracy 96.875%\n",
      "step 894 loss 0.7953843726637729, accuracy 94.53125%\n",
      "step 895 loss 0.9035230199950802, accuracy 93.75%\n",
      "step 896 loss 0.5869527585989667, accuracy 97.65625%\n",
      "step 897 loss 0.6897603071516888, accuracy 98.4375%\n",
      "step 898 loss 0.743206297234185, accuracy 94.53125%\n",
      "step 899 loss 0.5954776209019886, accuracy 96.875%\n",
      "step 900 loss 1.1592711038901717, accuracy 92.96875%\n",
      "step 901 loss 1.0047410968532455, accuracy 92.1875%\n",
      "step 902 loss 1.0525522859625187, accuracy 93.75%\n",
      "step 903 loss 0.8310901734321477, accuracy 92.96875%\n",
      "step 904 loss 1.1374853641496088, accuracy 91.40625%\n",
      "step 905 loss 0.8920539470463201, accuracy 94.53125%\n",
      "step 906 loss 1.1883561113999348, accuracy 93.75%\n",
      "step 907 loss 0.840638216191798, accuracy 94.53125%\n",
      "step 908 loss 0.8064570498175516, accuracy 90.625%\n",
      "step 909 loss 0.9397974762383521, accuracy 89.84375%\n",
      "step 910 loss 1.0480171419405742, accuracy 94.53125%\n",
      "step 911 loss 0.7290130496438793, accuracy 94.53125%\n",
      "step 912 loss 0.6103414780647622, accuracy 97.65625%\n",
      "step 913 loss 0.9427370677222174, accuracy 87.5%\n",
      "step 914 loss 0.8721695913069286, accuracy 94.53125%\n",
      "step 915 loss 0.7764029214450383, accuracy 96.09375%\n",
      "step 916 loss 0.4956335110424175, accuracy 95.3125%\n",
      "step 917 loss 0.7157975574940397, accuracy 95.3125%\n",
      "step 918 loss 0.7132799203671121, accuracy 94.53125%\n",
      "step 919 loss 1.1841549952047914, accuracy 92.96875%\n",
      "step 920 loss 0.763497341434365, accuracy 92.1875%\n",
      "step 921 loss 1.0921698307926417, accuracy 92.96875%\n",
      "step 922 loss 0.738555484258866, accuracy 97.65625%\n",
      "step 923 loss 0.9747302979944924, accuracy 93.75%\n",
      "step 924 loss 0.872346000758801, accuracy 91.40625%\n",
      "step 925 loss 0.8228386649370477, accuracy 94.53125%\n",
      "step 926 loss 0.5102936675315931, accuracy 96.09375%\n",
      "step 927 loss 0.6355002642688278, accuracy 97.65625%\n",
      "step 928 loss 0.9761293723195618, accuracy 92.96875%\n",
      "step 929 loss 0.8363108713654126, accuracy 90.625%\n",
      "step 930 loss 0.7857002193744409, accuracy 93.75%\n",
      "step 931 loss 0.6154896163338767, accuracy 96.09375%\n",
      "step 932 loss 0.9160517436981316, accuracy 95.3125%\n",
      "step 933 loss 0.540944727739805, accuracy 95.3125%\n",
      "step 934 loss 0.735903577311621, accuracy 94.53125%\n",
      "step 935 loss 1.2065268414310093, accuracy 91.40625%\n",
      "step 936 loss 0.7864827208068347, accuracy 96.875%\n",
      "step 937 loss 0.8259247763648628, accuracy 91.40625%\n",
      "step 938 loss 0.983717230187795, accuracy 96.09375%\n",
      "step 939 loss 1.2682811451207316, accuracy 89.0625%\n",
      "step 940 loss 0.6600373577165914, accuracy 95.3125%\n",
      "step 941 loss 0.798640421053486, accuracy 93.75%\n",
      "step 942 loss 1.0947702520139895, accuracy 90.625%\n",
      "step 943 loss 0.7077323012559515, accuracy 94.53125%\n",
      "step 944 loss 0.74900783040282, accuracy 97.65625%\n",
      "step 945 loss 0.8104065348747679, accuracy 94.53125%\n",
      "step 946 loss 0.8683883831905974, accuracy 96.09375%\n",
      "step 947 loss 0.7869263448418831, accuracy 95.3125%\n",
      "step 948 loss 0.6497581066032588, accuracy 96.09375%\n",
      "step 949 loss 0.9857023537558749, accuracy 91.40625%\n",
      "step 950 loss 0.9879612798360926, accuracy 94.53125%\n",
      "step 951 loss 0.8674370910706999, accuracy 93.75%\n",
      "step 952 loss 0.5853688427089406, accuracy 96.875%\n",
      "step 953 loss 0.6692445741014798, accuracy 96.09375%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 954 loss 0.6355142528775524, accuracy 96.09375%\n",
      "step 955 loss 0.9275991270422779, accuracy 92.96875%\n",
      "step 956 loss 0.7902841892324917, accuracy 94.53125%\n",
      "step 957 loss 0.49097852966744776, accuracy 97.65625%\n",
      "step 958 loss 0.8969219322969099, accuracy 94.53125%\n",
      "step 959 loss 0.5378341756093855, accuracy 96.875%\n",
      "step 960 loss 0.9112755079101527, accuracy 92.96875%\n",
      "step 961 loss 0.6958480226915444, accuracy 92.96875%\n",
      "step 962 loss 0.7134680972761396, accuracy 92.96875%\n",
      "step 963 loss 0.7755068241587127, accuracy 94.53125%\n",
      "step 964 loss 0.7940711940976808, accuracy 92.96875%\n",
      "step 965 loss 0.7556470502996749, accuracy 93.75%\n",
      "step 966 loss 0.6787043619353003, accuracy 93.75%\n",
      "step 967 loss 0.8293779842856203, accuracy 93.75%\n",
      "step 968 loss 0.6744806379305307, accuracy 97.65625%\n",
      "step 969 loss 0.8971622199369838, accuracy 94.53125%\n",
      "step 970 loss 0.8379861106495573, accuracy 94.53125%\n",
      "step 971 loss 0.8698520841724567, accuracy 93.75%\n",
      "step 972 loss 0.677281582309532, accuracy 96.09375%\n",
      "step 973 loss 0.8587899814391259, accuracy 93.75%\n",
      "step 974 loss 0.7783788288168406, accuracy 96.09375%\n",
      "step 975 loss 0.7903808562280156, accuracy 95.3125%\n",
      "step 976 loss 0.5261813097004762, accuracy 96.09375%\n",
      "step 977 loss 0.6189415009480124, accuracy 96.09375%\n",
      "step 978 loss 0.9533958145450424, accuracy 93.75%\n",
      "step 979 loss 0.7608923997259189, accuracy 93.75%\n",
      "step 980 loss 0.7542764652328307, accuracy 96.875%\n",
      "step 981 loss 1.2959822656570932, accuracy 92.96875%\n",
      "step 982 loss 0.9452495722045282, accuracy 92.1875%\n",
      "step 983 loss 0.7595061307840069, accuracy 93.75%\n",
      "step 984 loss 0.922497903355318, accuracy 95.3125%\n",
      "step 985 loss 0.9267616097404919, accuracy 92.1875%\n",
      "step 986 loss 0.7773881261995683, accuracy 94.53125%\n",
      "step 987 loss 0.6678297290679601, accuracy 96.09375%\n",
      "step 988 loss 0.7250774358849984, accuracy 94.53125%\n",
      "step 989 loss 0.6733542602999033, accuracy 96.09375%\n",
      "step 990 loss 0.7897580139555906, accuracy 94.53125%\n",
      "step 991 loss 0.764201881151197, accuracy 92.1875%\n",
      "step 992 loss 0.8165054650601297, accuracy 92.96875%\n",
      "step 993 loss 0.8996484327568224, accuracy 95.3125%\n",
      "step 994 loss 0.7297902300849669, accuracy 96.09375%\n",
      "step 995 loss 1.001308822342964, accuracy 92.1875%\n",
      "step 996 loss 0.7859010165875543, accuracy 95.3125%\n",
      "step 997 loss 0.8490750804316048, accuracy 90.625%\n",
      "step 998 loss 1.0376835614388082, accuracy 90.625%\n",
      "step 999 loss 0.8086089994001703, accuracy 96.09375%\n",
      "Value(data=0.8070432851626528, grad=0) 0.9415692821368948\n"
     ]
    }
   ],
   "source": [
    "# optimization\n",
    "step_cnt = 1000\n",
    "for k in range(step_cnt):\n",
    "    \n",
    "    # forward\n",
    "    total_loss, acc = loss(batch_size=128)\n",
    "    \n",
    "    # backward\n",
    "    model.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # update (sgd)\n",
    "    learning_rate = 0.1* (1.0 - 0.9*k/step_cnt)\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    \n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")\n",
    "\n",
    "        \n",
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
